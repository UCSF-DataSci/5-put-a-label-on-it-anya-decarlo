{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d45399",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to Classification\n",
    "    \n",
    "This notebook implements a basic classification model for health data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276e0fc",
   "metadata": {},
   "source": [
    "## Function: load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefea420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the synthetic health data from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the data with timestamp column converted to datetime\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found at: {file_path}\")\n",
    "    \n",
    "    # Load the CSV file using pandas\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert timestamp column to datetime if it exists\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        print(f\"Converted timestamp column to datetime format\")\n",
    "    \n",
    "    print(f\"Data loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa8457",
   "metadata": {},
   "source": [
    "## Function: prepare_data_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14313bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_part1(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling: select features, split into train/test sets, handle missing values.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        test_size: Proportion of data for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # 1. Select relevant features (age, systolic_bp, diastolic_bp, glucose_level, bmi)\n",
    "    features = ['age', 'systolic_bp', 'diastolic_bp', 'glucose_level', 'bmi']\n",
    "    X = df[features]\n",
    "    \n",
    "    # 2. Select target variable (disease_outcome)\n",
    "    y = df['disease_outcome']\n",
    "    \n",
    "    # 3. Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 4. Handle missing values using SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    X_test = pd.DataFrame(\n",
    "        imputer.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Data prepared: {X_train.shape[0]} training samples, {X_test.shape[0]} testing samples\")\n",
    "    print(f\"Features: {', '.join(features)}\")\n",
    "    print(f\"Target distribution - Training: {np.bincount(y_train)}, Testing: {np.bincount(y_test)}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696025f6",
   "metadata": {},
   "source": [
    "## Function: train_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, random_state=42, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        random_state: Random seed for reproducibility\n",
    "        max_iter: Maximum number of iterations for convergence\n",
    "        \n",
    "    Returns:\n",
    "        Trained logistic regression model\n",
    "    \"\"\"\n",
    "    # Initialize logistic regression model with balanced class weights\n",
    "    # This helps account for any class imbalance in the data\n",
    "    model = LogisticRegression(\n",
    "        random_state=random_state,\n",
    "        max_iter=max_iter,\n",
    "        class_weight='balanced',  # Adjust weights inversely proportional to class frequencies\n",
    "        solver='lbfgs'  # Efficient solver for small datasets\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Print model information\n",
    "    print(f\"Logistic Regression model trained with {X_train.shape[1]} features\")\n",
    "    print(f\"Model coefficients: {model.coef_}\")\n",
    "    print(f\"Model intercept: {model.intercept_}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563db70",
   "metadata": {},
   "source": [
    "## Function: calculate_evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calculate classification evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing accuracy, precision, recall, f1, auc, and confusion_matrix\n",
    "    \"\"\"\n",
    "    # 1. Generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # For metrics like AUC, we need probability predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "    \n",
    "    # 2. Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()  # Convert to list for serialization\n",
    "    }\n",
    "    \n",
    "    # 3. Print metrics summary\n",
    "    for metric_name, value in metrics.items():\n",
    "        if metric_name != 'confusion_matrix':\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TN: {metrics['confusion_matrix'][0][0]}, FP: {metrics['confusion_matrix'][0][1]}\")\n",
    "    print(f\"FN: {metrics['confusion_matrix'][1][0]}, TP: {metrics['confusion_matrix'][1][1]}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f621a",
   "metadata": {},
   "source": [
    "## Function: save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5619535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(metrics, output_file='results/results_part1.txt'):\n",
    "    \"\"\"\n",
    "    Save evaluation metrics to a text file.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        output_file: Path to save the results\n",
    "    \"\"\"\n",
    "    # 1. Create results directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # 2. Create two versions of the output:\n",
    "    # - A human-readable version with full formatting\n",
    "    # - A test-compatible version with simple key:value pairs\n",
    "    \n",
    "    # Create the full formatted version for humans to read\n",
    "    formatted_output_file = output_file.replace('.txt', '_formatted.txt')\n",
    "    with open(formatted_output_file, 'w') as f:\n",
    "        f.write(\"Classification Model Evaluation Metrics\\n\")\n",
    "        f.write(\"====================================\\n\\n\")\n",
    "        \n",
    "        # Write simple metrics first\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name != 'confusion_matrix':\n",
    "                f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "        \n",
    "        # Write confusion matrix with labels\n",
    "        f.write(\"\\nConfusion Matrix:\\n\")\n",
    "        cm = metrics['confusion_matrix']\n",
    "        f.write(\"              Predicted:\\n\")\n",
    "        f.write(\"              Negative  Positive\\n\")\n",
    "        f.write(f\"Actual: Negative  {cm[0][0]:8d}  {cm[0][1]:8d}\\n\")\n",
    "        f.write(f\"        Positive  {cm[1][0]:8d}  {cm[1][1]:8d}\\n\")\n",
    "    \n",
    "    # Create the simple version for tests that expect key:value pairs\n",
    "    with open(output_file, 'w') as f:\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name != 'confusion_matrix':\n",
    "                f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "    \n",
    "    # Also save as JSON for easier programmatic comparison\n",
    "    import json\n",
    "    # Handle different types of confusion matrix formats safely\n",
    "    metrics_for_json = {}\n",
    "    for k, v in metrics.items():\n",
    "        if k != 'confusion_matrix':\n",
    "            metrics_for_json[k] = float(v)\n",
    "        else:\n",
    "            # Handle different formats of confusion matrix\n",
    "            if hasattr(v, 'tolist'):\n",
    "                metrics_for_json[k] = v.tolist()\n",
    "            else:\n",
    "                # Already a list or another format\n",
    "                metrics_for_json[k] = v\n",
    "                \n",
    "    json_output_file = output_file.replace('.txt', '.json')\n",
    "    with open(json_output_file, 'w') as f:\n",
    "        json.dump(metrics_for_json, f)\n",
    "    \n",
    "    print(f\"Results saved to {output_file} (test format) and {formatted_output_file} (human-readable)\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193b65f",
   "metadata": {},
   "source": [
    "## Function: interpret_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0713b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_results(metrics):\n",
    "    \"\"\"\n",
    "    Analyze model performance on imbalanced data.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "        - 'best_metric': Name of the metric that performed best\n",
    "        - 'worst_metric': Name of the metric that performed worst\n",
    "        - 'imbalance_impact_score': A score from 0-1 indicating how much\n",
    "          the class imbalance affected results (0=no impact, 1=severe impact)\n",
    "    \"\"\"\n",
    "    # Extract the metrics we want to compare (excluding confusion_matrix)\n",
    "    comparable_metrics = {k: v for k, v in metrics.items() if k != 'confusion_matrix'}\n",
    "    \n",
    "    # 1. Determine which metric performed best and worst\n",
    "    best_metric = max(comparable_metrics, key=comparable_metrics.get)\n",
    "    worst_metric = min(comparable_metrics, key=comparable_metrics.get)\n",
    "    \n",
    "    # 2. Calculate an imbalance impact score\n",
    "    # Logic: Compare accuracy (which can be misleadingly high on imbalanced data)\n",
    "    # with more robust metrics like F1 or recall\n",
    "    imbalance_sensitive_metrics = ['f1', 'recall']\n",
    "    min_sensitive_value = min(metrics[m] for m in imbalance_sensitive_metrics)\n",
    "    \n",
    "    # Calculate the normalized difference between accuracy and the minimum of F1/recall\n",
    "    # This serves as our imbalance impact score - higher means more impact\n",
    "    accuracy = metrics['accuracy']\n",
    "    imbalance_impact_score = abs(accuracy - min_sensitive_value) / max(accuracy, 0.001)\n",
    "    \n",
    "    # Clip to ensure it's between 0 and 1\n",
    "    imbalance_impact_score = min(1.0, max(0.0, imbalance_impact_score))\n",
    "    \n",
    "    # 3. Return the results\n",
    "    interpretation = {\n",
    "        'best_metric': best_metric,\n",
    "        'worst_metric': worst_metric,\n",
    "        'imbalance_impact_score': float(imbalance_impact_score)  # Ensure it's a Python float\n",
    "    }\n",
    "    \n",
    "    # Print interpretation for user feedback\n",
    "    print(f\"Best performing metric: {best_metric} ({metrics[best_metric]:.4f})\")\n",
    "    print(f\"Worst performing metric: {worst_metric} ({metrics[worst_metric]:.4f})\")\n",
    "    print(f\"Class imbalance impact score: {imbalance_impact_score:.4f} \" + \n",
    "          f\"({'Low' if imbalance_impact_score < 0.3 else 'Moderate' if imbalance_impact_score < 0.6 else 'High'} impact)\")\n",
    "    \n",
    "    return interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1d733",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8096746",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Load data\n",
    "    data_file = 'data/synthetic_health_data.csv'\n",
    "    df = load_data(data_file)\n",
    "    \n",
    "    # 2. Prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data_part1(df)\n",
    "    \n",
    "    # 3. Train model\n",
    "    model = train_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    # 4. Evaluate model\n",
    "    metrics = calculate_evaluation_metrics(model, X_test, y_test)\n",
    "    \n",
    "    # 5. Save results\n",
    "    output_file = save_results(metrics)\n",
    "    \n",
    "    # 6. Interpret results\n",
    "    interpretation = interpret_results(metrics)\n",
    "    print(\"\\nResults Interpretation:\")\n",
    "    for key, value in interpretation.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
